{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep autoencoders for recommendations\n",
    "\n",
    "In this notebook, we'll apply a more advanced algorithm to the same dataset as before, taking a different approach. We'll use a deep autoencoder network, which attempts to reconstruct its input and with that gives us ratings for unseen user / movie pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the logdir if it exists\n",
    "import shutil\n",
    "shutil.rmtree('logs', ignore_errors=True)\n",
    "\n",
    "# Load TensorBoard extension for notebooks\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_ratings_file = 'https://github.com/janhartman/recsystf/raw/master/datasets/movielens_ratings.csv'\n",
    "df = pd.read_csv(movielens_ratings_file)\n",
    "\n",
    "user_ids = df['userId'].unique()\n",
    "user_encoding = {x: i for i, x in enumerate(user_ids)}   # {user_id: index}\n",
    "movie_ids = df['movieId'].unique()\n",
    "movie_encoding = {x: i for i, x in enumerate(movie_ids)}\n",
    "\n",
    "df['user'] = df['userId'].map(user_encoding)    # Map from IDs to indices\n",
    "df['movie'] = df['movieId'].map(movie_encoding)\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the same dataset as before, but our preprocessing will be a bit different due to the difference in our model. Our autoencoder will take a vector of all ratings for a movie and attempt to reconstruct it. However, our input vector will have a lot of zeroes due to the sparsity of our data. We'll modify our loss so our model won't predict zeroes for those combinations - it will actually predict unseen ratings. \n",
    "\n",
    "To facilitate this, we'll use the sparse tensor that TF supports. Note: to make training easier, we'll transform it to dense form, which would not work in larger datasets - we would have to preprocess the data in a different way or stream it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sparse tensor: at each user, movie location, we have a value, the rest is 0\n",
    "x = tf.sparse.SparseTensor(indices=df[['movie', 'user']].values, values=df['rating'], dense_shape=(n_movies, n_users))\n",
    "\n",
    "# Transform it to dense form and to float32 (good enough precision)\n",
    "x = tf.cast(tf.sparse.to_dense(tf.sparse.reorder(x)), tf.float32)\n",
    "\n",
    "# Shuffle the data\n",
    "x = tf.random.shuffle(x, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the model. We'll have to specify the input shape. Because we have 9724 movies and only 610 users, we'll prefer to predict ratings for movies instead of users - this way, our dataset is larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = n_users\n",
    "inputs = layers.Input(shape=(n,))\n",
    "\n",
    "encoder1 = layers.Dense(28, activation='selu', kernel_initializer='glorot_uniform')\n",
    "encoder2 = layers.Dense(56, activation='selu', kernel_initializer='glorot_uniform')\n",
    "code = layers.Dense(56, activation='selu', kernel_initializer='glorot_uniform')\n",
    "dropout = layers.Dropout(0.65)\n",
    "decoder1 = layers.Dense(56, activation='selu', kernel_initializer='glorot_uniform')\n",
    "decoder2 = layers.Dense(28, activation='selu', kernel_initializer='glorot_uniform')\n",
    "out = layers.Dense(n, activation='selu', kernel_initializer='glorot_uniform')\n",
    "\n",
    "encoded1 = encoder1(inputs)\n",
    "encoded2 = encoder2(encoded1)\n",
    "coded = code(encoded2)\n",
    "dropped = dropout(coded)\n",
    "decoded1 = decoder1(dropped)\n",
    "decoded2 = decoder2(decoded1)\n",
    "temp_output = out(decoded2)\n",
    "\n",
    "re_encoded1 = encoder1(temp_output)\n",
    "re_encoded2 = encoder2(re_encoded1)\n",
    "re_coded = code(re_encoded2)\n",
    "re_dropped = dropout(re_coded)\n",
    "re_decoded1 = decoder1(re_dropped)\n",
    "re_decoded2 = decoder2(re_decoded1)\n",
    "output = out(re_decoded2)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our inputs are sparse, we'll need to create a modified mean squared error function. We have to look at which ratings are zero in the ground truth and remove them from our loss calculation (if we didn't, our model would quickly learn to predict zeros almost everywhere).\n",
    "We'll use masking - first get a boolean mask of non-zero values and then extract them from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(y_true, y_pred):\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    se = tf.boolean_mask(tf.square(y_true - y_pred), mask)\n",
    "    return tf.reduce_mean(se)\n",
    "\n",
    "model.compile(\n",
    "    loss=masked_mse,\n",
    "    optimizer=keras.optimizers.Adam()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model training will be similar as before - we'll use early stopping and TensorBoard. Our batch size will be smaller due to the lower number of examples. Note that we are passing the same array for both $x$ and $y$, because the autoencoder reconstructs its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=1e-2,\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(log_dir='logs')\n",
    "]\n",
    "\n",
    "model.fit(x, \n",
    "          x, \n",
    "          batch_size=16, \n",
    "          epochs=100, \n",
    "          validation_split=0.2,\n",
    "          callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our loss and the model itself with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We've seen how to use TensorFlow to implement recommender systems in a few different ways. I hope this short introduction has been informative and has prepared you to use TF on new problems. Thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
