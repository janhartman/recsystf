{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix factorization\n",
    "\n",
    "In this notebook, we'll finally start implementing recommender system algorithms. First, we'll look at a simple dataset, then implement the matrix factorization model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the logdir if it exists\n",
    "import shutil\n",
    "shutil.rmtree('logs', ignore_errors=True)\n",
    "\n",
    "# Load TensorBoard extension for notebooks\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's download the data and see how it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens_ratings_file = 'https://github.com/janhartman/recsystf/raw/master/datasets/movielens_ratings.csv'\n",
    "df = pd.read_csv(movielens_ratings_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess the data. We'll encode each unique user and movie with an index from 0 to $n-1$. We could reuse already existing IDs, but it's better to have our own mapping (+ this approach is more general and would also work for e.g. names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = df['userId'].unique()\n",
    "user_encoding = {x: i for i, x in enumerate(user_ids)}   # {user_id: index}\n",
    "movie_ids = df['movieId'].unique()\n",
    "movie_encoding = {x: i for i, x in enumerate(movie_ids)}\n",
    "\n",
    "df['user'] = df['userId'].map(user_encoding)    # Map from IDs to indices\n",
    "df['movie'] = df['movieId'].map(movie_encoding)\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)\n",
    "\n",
    "min_rating = min(df['rating'])\n",
    "max_rating = max(df['rating'])\n",
    "\n",
    "print(f'Number of users: {n_users}\\nNumber of movies: {n_movies}\\nMin rating: {min_rating}\\nMax rating: {max_rating}')\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll build the model. We'll do this in a new way: by subclassing the `Model` class and implementing the necessary functionality. Note: this way of building is typically not recommended if you're not looking to significantly change a model's behavior - we're showing it for completeness' sake. \n",
    "\n",
    "The implementation is divided into the creation and application of the layers and is otherwise quite similar to what we've already seen. The most important layer we'll be using is the `Embedding` layer. It is essentially a look-up array of weights, where each row is an entity (user or movie) represented with N latent dimensions. Its inputs have to be integer indices on the range $[0, N-1]$, which is why we encoded the IDs. Then, a call of this layer with an index $i$ returns the $i$-th row of this matrix. This is a great way of handling sparse inputs which are omnipresent in recommender systems. At Zemanta, we also utilize this layer in our models. Essentially, we are decomposing the sparse user-matrix rating matrix into two dense matrices held in two of these embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(models.Model):\n",
    "    def __init__(self, n_users, n_movies, n_factors, **kwargs):\n",
    "        super(Recommender, self).__init__(**kwargs)\n",
    "        self.n_users = n_users\n",
    "        self.n_movies = n_movies\n",
    "        self.n_factors = n_factors\n",
    "        \n",
    "        # We specify the size of the matrix,\n",
    "        # the initializer (truncated normal distribution)\n",
    "        # and the regularization type and strength (L2 with lambda = 1e-6)\n",
    "        self.user_emb = layers.Embedding(n_users, \n",
    "                                         n_factors, \n",
    "                                         embeddings_initializer='he_normal',\n",
    "                                         embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                         name='user_embedding')\n",
    "        self.movie_emb = layers.Embedding(n_movies, \n",
    "                                          n_factors, \n",
    "                                          embeddings_initializer='he_normal',\n",
    "                                          embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                          name='movie_embedding')\n",
    "        \n",
    "        # Embedding returns a 3D tensor with one dimension = 1, so we reshape it to a 2D tensor\n",
    "        self.reshape = layers.Reshape((self.n_factors,))\n",
    "        \n",
    "        # Dot product of the latent vectors\n",
    "        self.dot = layers.Dot(axes=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Two inputs\n",
    "        user, movie = inputs\n",
    "        u = self.user_emb(user)\n",
    "        u = self.reshape(u)\n",
    "    \n",
    "        m = self.movie_emb(movie)\n",
    "        m = self.reshape(m)\n",
    "        \n",
    "        return self.dot([u, m])\n",
    "\n",
    "n_factors = 50\n",
    "model = Recommender(n_users, n_movies, n_factors)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "    loss=keras.losses.MeanSquaredError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.summary()\n",
    "except ValueError as e:\n",
    "    print(e, type(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why building models via subclassing is a bit annoying - you can run into errors such as this. We'll fix it by calling the model with some fake data so it knows the shapes of the inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model([np.array([1, 2, 3]), np.array([2, 88, 5])])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to expand our toolbox by introducing callbacks. Callbacks can be used to monitor our training progress, decay the learning rate, periodically save the weights or even stop early in case of detected overfitting. In Keras, they are really easy to use: you just create a list of desired callbacks and pass it to the `model.fit` method. It's also really easy to define your own by subclassing the `Callback` class. You can also specify when they will be triggered - the default is at the end of every epoch.\n",
    "\n",
    "We'll use two: an early stopping callback which will monitor our loss and stop the training early if needed and TensorBoard, a utility for visualizing models, monitoring the training progress and much more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor='val_loss',\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard(log_dir='logs')\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x=(df['user'].values, df['movie'].values),  # The model has two inputs!\n",
    "    y=df['rating'],\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we stopped early because the validation loss was not improving. Now, we'll open TensorBoard (it's a separate program called via command-line) to read the written logs and visualize the loss over all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TensorBoard and specify the log dir\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how easy it is to implement a recommender system with Keras and use a few utilities to make it easier to experiment. Note that this model is still quite basic and we could easily improve it: we could try adding a bias for each user and movie or adding non-linearity by using a sigmoid function and then rescaling the output. It could also be extended to use other features of a user or movie. \n",
    "\n",
    "Next, we'll try a bigger, more state-of-the-art model: a deep autoencoder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
