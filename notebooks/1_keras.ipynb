{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Keras is fairly well-known in the Python deep learning community. It used to be a high-level API to make frameworks like CNTK, Theano and TensorFlow easier to use and was framework-agnostic (you only had to set the backend for processing, everything else was abstracted). A few years ago, Keras was migrated to the TF repository and dropped support for other backends. It is now the de-facto high level API for TF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers and models\n",
    "The basic component of a Keras model is a layer. A layer is comprised of one or more operations and is meant to be easily reusable. A model is a graph of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "tf.random.set_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the well-known Boston housing dataset. It is a small dataset of 500 examples (homes in Boston) with 13 features where the target variable is the price of a home. Keras has this dataset easily accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boston housing dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data()\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test  shape: {y_test.shape}')\n",
    "\n",
    "# Preprocess the data (these are Numpy arrays)\n",
    "# Standardize by subtracting the mean and dividing with the standard deviation\n",
    "x_mean = np.mean(x_train, axis=0)\n",
    "x_std = np.std(x_train, axis=0)\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "n_features = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a linear regression model\n",
    "You'll see examples in the TF guide immediately start with neural networks. We will begin with a linear regression model and then move onto a simple neural network, showing that it is very easy to construct both.\n",
    "\n",
    "The simplest way to construct a Keras model is a sequential model, which is just a sequence of layers (or operations). For simple models, this works well. Our linear regression model has just one layer - a [dense layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), which means a multiplication with a weight and a bias addition. This is just the equation $y = Wx + b$.\n",
    "\n",
    "This layer can also be used for neural networks, where we would be multiplying matrices. We choose `units = 1` (output a vector of size 1, i.e. a scalar for each example) and `input_shape = (n_features, )`, which means that we have 13 features for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = layers.Dense(\n",
    "    units=1,\n",
    "    input_shape=(n_features,),\n",
    "    name='linear_layer'\n",
    ")\n",
    "\n",
    "linear_model = models.Sequential([\n",
    "    linear_layer\n",
    "], name='linear_model')\n",
    "\n",
    "# View the weights (parameters) of a layer\n",
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look a very useful method which gives us the summary of a model. It shows us all the layers + their shapes and the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling and training the model\n",
    "Before training, we need to specify the optimizer we will use and which loss we want to minimize. We'll pick the simplest options: stochastic gradient descent for the optimizer and mean squared error for the loss function. We do this by \"compiling\" the model, which prepares it for training. After that, we can call `.fit`, which is similar to scikit-learn. However, here we have to specify the number of iterations (epochs) we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.compile(\n",
    "    # Optimizer: Stochastic gradient descent\n",
    "    optimizer=keras.optimizers.SGD(),  \n",
    "    # Loss function to minimize: mean squared error\n",
    "    loss=keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "linear_model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    batch_size=0, # Use all data as one batch\n",
    "    verbose=0,    # Don't print progress\n",
    "    epochs=20     # Run 20 iterations of gradient updates \n",
    ")\n",
    "\n",
    "y_hat = linear_model.predict(x_test)\n",
    "\n",
    "mse = keras.metrics.MeanSquaredError()(y_test, y_hat)\n",
    "print(f'Test MSE: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we also managed to get a decent fit (MSE around 20 should be OK for this dataset). Observe how we managed to do this by avoiding all optimization-related mathematics - we only had to specify how the model computes predictions (the forward pass). Since our model and optimization procedure were very simple, the code isn't really much more complicated than if we used lower-level tools, but for more advanced approaches, it's usually much simpler to use Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details for layers and models\n",
    "\n",
    "A layer holds state and performs computations. Layers should be easily reusable and composable - you can create more complex layers from simpler ones.\n",
    "A model is a graph of layers and offers train / predict / evaluate APIs. It is easily serializable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API example\n",
    "A core principle of Keras is the progressive increase of complexity. You should always be able to get into lower-level workflows in a gradual way. You shouldn't fall off a cliff if the high-level functionality doesn't exactly match your use case. You should be able to gain more control over the small details while keeping some high-level convenience.\n",
    "\n",
    "For more sophisticated usecases, we might want a more flexible way to construct models (recurrent models, multiple inputs and outputs...). We'll move onto the Functional API, which offers us just that. Instead of specifying a sequence of layers, we create each layer individually and apply it onto the previous ones. This is the most popular way of creating models in Keras and is recommended. Each layer is a Python class which you apply by calling it with its inputs. The inputs to each layer are tensors (`tf.Tensor`). Once you have all the layers applied, you simply create a model by telling it what is its input and its output. TensorFlow will find the path through the graph of layers you have created and note down which layers are part of the graph (unused parts will not be included).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural networks\n",
    "Now, we'll move on to a neural network example. We will apply a neural network with a small hidden layer to the same dataset. Due to the simplicity of Keras, this won't be much more difficult than the linear regression example! \n",
    "\n",
    "We'll use only dense layers and the ReLU activation function. In this example, our inputs will have 13 features. With matrix multiplication in the dense layers, we will transform them to a 3-dim and 1-dimensional space, which will be the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start with the Input class, which is a placeholder for the actual data we'll send in. The shape we are specifying is the number of features, i.e. length of each vector (example). Even though we can (and should) feed in matrices of data, the batch size (the number of rows in the matrix) is always omitted in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(n_features,), name='inputs')\n",
    "\n",
    "# After we create a layer object, we get its output by calling it and passing the input tensor.\n",
    "# The activation function can also be added by creating a `layers.ReLU` or `layers.Activation` layer.\n",
    "layer1 = layers.Dense(3, activation='relu', name='dense_1')\n",
    "x1 = layer1(inputs)\n",
    "\n",
    "# Our outputs will be prices (a single value for each example)\n",
    "layer2 = layers.Dense(1, name='dense_2')\n",
    "predictions = layer2(x1)\n",
    "\n",
    "nn_model = models.Model(inputs=inputs, outputs=predictions, name='nn_model')\n",
    "\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access input / output / weights attributes on both layers and models to see what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Layer 1 inputs: {layer1.input}\\n')\n",
    "print(f'Layer 1 outputs: {layer1.output}\\n')\n",
    "print(f'Model inputs: {nn_model.input}\\n')\n",
    "print(f'Model outputs: {nn_model.output}\\n')\n",
    "print(f'Layer 1 weights: {layer1.weights}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll need to compile the model before training. We'll use a different optimizer this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(999)\n",
    "\n",
    "nn_model.compile(\n",
    "    # Adam optimizer, better suited for neural networks\n",
    "    optimizer=keras.optimizers.Adam(0.15),  \n",
    "    # Loss function to minimize: mean squared error\n",
    "    loss=keras.losses.MeanSquaredError()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use a validation set (we don't train on it) to monitor the loss during training. Although training is typically done in minibatches, we will use the whole dataset here as it is fairly small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = nn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=0,         # The minibatch size (tradeoff between speed and quality) - use all data\n",
    "    epochs=20,            \n",
    "    validation_split=0.1  # How much of the training data to use for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.fit` method returns an object which contains the losses for each epoch during training. You can use it to save them or plot them, but we will show an easier and more powerful tool for this in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the loss and metrics history\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation results seem good. To make sure, we'll also evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "mse = nn_model.evaluate(x_test, y_test, batch_size=0)\n",
    "print(f'Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our test loss is higher than our validation loss, but overall still OK, even lower than the linear regression loss. Since we're dealing with a small amount of data, overfitting can easily happen - we'll demonstrate some useful techniques to combat it in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further\n",
    "The [Keras API docs](https://www.tensorflow.org/api_docs/python/tf/keras) are pretty nice and contain a lot more material - definitely check out the list of layers. And even if you can't find something you need - it's easy to create your own layer, metric or loss function with plain TF operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
