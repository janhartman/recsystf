{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Keras is fairly well-known in the Python deep learning community. It used to be a high-level API to make frameworks like CNTK, Theano and TensorFlow easier to use and was framework-agnostic (you only had to set the backend for processing, everything else was abstracted). A few years ago, Keras was migrated to the TF repository and dropped support for other backends. It is now the de-facto high level API for TF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers and models\n",
    "The basic component of a Keras model is a layer. A layer is comprised of one or more operations and is meant to be easily reusable. A model is a graph of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see examples in the TF guide immediately start with neural networks. My own choice is to first see how to implement something really simple, like linear regression again.\n",
    "\n",
    "The simplest way to construct a Keras model is a sequential model, which is just a sequence of layers (or operations). For simple models, this works well. Our linear regression model has just one layer - a dense layer, which means a multiplication with a weight and a bias addition. This layer can also be used for neural networks, where we would be multiplying matrices. We choose `units = 1` (output a vector of size 1, i.e. a scalar for each example) and `input_shape = (1, )`, which means that we have 1 feature for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = layers.Dense(\n",
    "    units=1, \n",
    "    input_shape=(1,),\n",
    "    name=\"linear_layer\"\n",
    ")\n",
    "\n",
    "linear_model = models.Sequential([\n",
    "    linear_layer\n",
    "], name=\"linear_model\")\n",
    "\n",
    "# View the weights (parameters) of a layer\n",
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look a very useful method which gives us the summary of a model. It shows us all the layers + their shapes and the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we need to specify the optimizer we will use and which loss we want to minimize. We'll choose the same as before: gradient descent and mean squared error. We do this by \"compiling\" the model, which prepares it for training. After that, we can call `.fit`, which is similar to scikit-learn. However, here we have to specify the number of iterations (epochs) we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.compile(\n",
    "    optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    ")\n",
    "\n",
    "# Same data as before\n",
    "x = tf.cast(tf.linspace(-1, 1, 21), tf.float32)\n",
    "y = x + tf.random.uniform(x.shape, -0.05, 0.05, seed=117) + 2\n",
    "\n",
    "# Run 50 iterations of GD updates\n",
    "linear_model.fit(x, y, verbose=0, epochs=50)\n",
    "\n",
    "y_hat = linear_model.predict(x)\n",
    "mse = keras.metrics.MeanSquaredError()(y, y_hat)\n",
    "print(f'MSE: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we also managed to get a good fit. Observe how we managed to do this by avoiding all optimization-related mathematics - we only had to specify how the model computes predictions (the forward pass). Since our model and optimization procedure were very simple, the code wasn't all that different, but for more advanced approaches, it's usually much simpler to use Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details for layers and models\n",
    "\n",
    "A layer holds state and performs computations. Layers should be easily reusable and composable - you can create more complex layers from simpler ones.\n",
    "A model is a graph of layers and offers train / predict / evaluate APIs. It is easily serializable.\n",
    "\n",
    "#### A custom layer example\n",
    "Let's write a really simple custom layer - the argmax function. We'll wrap the plain `tf.argmax` function to do the heavy lifting. This layer could come in handy if we had probabilities and wanted to extract the most probable class. Each custom layer class should inherit from `tf.keras.layers.Layer` and implement at least the `call()` method where the forward pass is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgMax(layers.Layer):\n",
    "    # This is where you would create weights/variables\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ArgMax, self).__init__(**kwargs)\n",
    "    \n",
    "    # Define the forward pass\n",
    "    def call(self, x):\n",
    "        return tf.argmax(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST example with the Functional API\n",
    "A core principle of Keras is the progressive increase of complexity. You should always be able to get into lower-level workflows in a gradual way. You shouldn't fall off a cliff if the high-level functionality doesn't exactly match your use case. You should be able to gain more control over the small details while keeping some high-level convenience.\n",
    "\n",
    "For more sophisticated usecases, we might want a more flexible way to construct models (recurrent models, multiple inputs and outputs...). We'll move onto the Functional API, which offers us just that. Instead of specifying a sequence of layers, we create each layer individually and apply it onto the previous ones. This is the most popular way of creating models in Keras and is recommended. Each layer is a Python class which you apply by calling it with its inputs. The inputs to each layer are tensors (`tf.Tensor`). Once you have all the layers applied, you simply create a model by telling it what is its input and its output. TensorFlow will find the path through the graph of layers you have created and note down which layers are part of the graph (unused parts will not be included).\n",
    "\n",
    "Now, we'll move on to a neural network example. We'll classify handwritten digits from the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) with a fully-connected neural network consisting of 3 layers. Due to the simplicity of Keras, this won't be much more difficult than the linear regression example! \n",
    "\n",
    "We need to start with the Input class, which is a placeholder for the actual data we'll send in. The shape we are specifying is the length of each vector (example). Even though we can (and should) feed in matrices of data, the batch size (the number of rows in the matrix) is always omitted in Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(784,), name=\"digits\")\n",
    "\n",
    "# After we create a layer object, we get its output by calling it and passing the input tensor.\n",
    "layer1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")\n",
    "x1 = layer1(inputs)\n",
    "\n",
    "# The activation function can also be added by creating a `layers.ReLU` or `layers.Activation` layer.\n",
    "layer2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")\n",
    "x2 = layer2(x1)\n",
    "\n",
    "# Our outputs will be probabilities for all 10 digits.\n",
    "# The softmax function transforms the outputs from real numbers to probabilities that sum to 1.\n",
    "proba_predictions = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x2)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=proba_predictions, name=\"mnist_model\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access input / output / weights attributes on both layers and models to see what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Layer 2: {layer2}\\n')\n",
    "print(f'Layer 2 inputs: {layer2.input} and outputs: {layer2.output}\\n')\n",
    "print(f'Model inputs: {model.input} and outputs: {model.output}\\n')\n",
    "print(f'Layer 2 weights: {layer2.weights}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has this dataset easily accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test  shape: {y_test.shape}')\n",
    "\n",
    "# Preprocess the data (these are Numpy arrays)\n",
    "# Reshape each matrix into a vector and rescale onto [0, 1]\n",
    "x_train = x_train.reshape(-1, 784).astype(np.float32) / 255\n",
    "x_test = x_test.reshape(-1, 784).astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll need to compile the model before training. We'll use a different optimizer and loss function this time. Lastly, we'll also add an accuracy metric so we can easily see how the training is going. Notice that we output 10 numbers for each example while the target is only one number - the class label. Luckily, the inbuilt sparse categorical loss and accuracy handle this for us out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # Optimizer: RMSprop\n",
    "    optimizer=keras.optimizers.RMSprop(),  \n",
    "    # Loss function to minimize: cross-entropy\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # List of metrics to monitor: accuracy\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have plenty of data, we'll use a validation set (we don't train on it) to monitor the loss and accuracy. We'll also perform the training in minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,        # The minibatch size, usually under 100 (tradeoff between speed and quality)\n",
    "    epochs=2,            \n",
    "    validation_split=0.2  # How much of the training data to use for validation\n",
    ")\n",
    "\n",
    "# Show the loss and metrics history\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation results seem good. To make sure, we'll also evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(f'Evaluation results: test loss {loss}, test accuracy {acc}\\n')\n",
    "\n",
    "# Generate predictions (probabilities - the output of the last layer) on new data\n",
    "predictions = model.predict(x_test[:10])\n",
    "print(f'Predictions {predictions[:3]} \\nwith shape {predictions[:3].shape}\\n')\n",
    "print(f'Ground truth: {y_test[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model gives us a vector of length 10 (the probability of each of the 10 digits). Looking through this is perhaps a bit cumbersome and we might want the most likely digit as an output by default. We could modify our model to output the digit with the highest probability, but we can also extend it by adding a new final layer. This gives us a new model that actually shares weights with the already trained model, which is quite useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luckily, we already implemented this layer :)\n",
    "# It gives us the index of the highest value in each row of the input, which is the most probable class of that row.\n",
    "argmax_layer = ArgMax(name=\"mnist_argmax\")\n",
    "\n",
    "# Reuse the already trained model by calling the argmax layer on its output\n",
    "model_class_pred = models.Model(inputs=inputs, outputs=argmax_layer(model.output), name=\"mnist_class_pred_model\")\n",
    "model_class_pred.summary()\n",
    "\n",
    "print(f'Class predictions: {model_class_pred.predict(x_test[:10])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our predictions in the form of class labels. Looking above, we can see that they look fairly close to the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further\n",
    "The [Keras API docs](https://www.tensorflow.org/api_docs/python/tf/keras) are pretty nice and contain a lot more material - definitely check out the list of layers. And even if you can't find something you need - it's easy to create your own layer, metric or loss function with plain TF operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
